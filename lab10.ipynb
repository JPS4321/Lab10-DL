{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd00a761",
   "metadata": {},
   "source": [
    "### **Laboratorio 10**\n",
    "\n",
    "- Juan Pablo Solis\n",
    "- Diego Garcia\n",
    "\n",
    "### **Link de github**\n",
    "https://github.com/JPS4321/Lab10_DL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1f896",
   "metadata": {},
   "source": [
    "## **Investigacion**\n",
    "\n",
    "### **SHAP**\n",
    "##### **Principio de Funcionamiento**\n",
    "\n",
    "El método SHAP se basa en la teoría de juegos cooperativos de Shapley, donde cada característica del modelo se considera como un “jugador” que contribuye al resultado final de la predicción. El objetivo es determinar cuánto aporta cada característica al resultado del modelo, tomando en cuenta todas las combinaciones posibles de variables. En términos prácticos, SHAP calcula la contribución promedio marginal de cada característica sobre la predicción final, permitiendo interpretar de forma justa y consistente cómo influyen las variables en la salida del modelo.\n",
    "\n",
    "##### **En que tipos de modelos se aplica**\n",
    "SHAP  puede aplicarse a cualquier tipo de algoritmo. Sin embargo, existen implementaciones más eficientes para modelos basados en árboles de decisión, como XGBoost, LightGBM o Random Forest. También puede utilizarse con modelos lineales, redes neuronales y máquinas de soporte vectorial (SVM). Su versatilidad lo convierte en una herramienta útil tanto en tareas de clasificación como de regresión.\n",
    "\n",
    "##### **Ejemplo**\n",
    "Supongamos un modelo que predice la probabilidad de que un cliente obtenga un crédito, utilizando las variables ingreso, edad y historial crediticio. El modelo predice una probabilidad de aprobación del 80%. Usando SHAP, se obtiene que el ingreso contribuyó con +0.25, la edad con +0.05 y el historial crediticio con +0.30 a esa predicción. Esto significa que las variables ingreso e historial crediticio fueron las que más influyeron positivamente en la decisión del modelo.\n",
    "\n",
    "### **LIME**\n",
    "##### **Principio de Funcionamiento**\n",
    "\n",
    "LIME busca explicar el comportamiento de un modelo complejo a nivel local, es decir, alrededor de una predicción específica. Lo hace generando pequeñas perturbaciones o variaciones de los datos de entrada y observando cómo cambia la predicción. Luego, ajusta un modelo simple y comprensible, como una regresión lineal o un árbol de decisión pequeño, para aproximar el comportamiento del modelo original en esa región local. De esta forma, LIME no intenta explicar todo el modelo, sino solo por qué tomó una decisión en un caso particular.\n",
    "\n",
    "##### **En que tipos de modelos se aplica**\n",
    "LIME puede aplicarse a modelos de clasificación o regresión, y es útil en algoritmos de caja negra como redes neuronales profundas, máquinas de soporte vectorial (SVM), random forest o boosting. Gracias a esta característica, LIME es uno de los métodos más utilizados para obtener explicaciones locales en sistemas de inteligencia artificial complejos.\n",
    "\n",
    "##### **Ejemplo**\n",
    "Imaginemos un modelo que clasifica correos electrónicos como “spam” o “no spam”. Para un correo específico, LIME genera múltiples versiones del mensaje modificando o eliminando algunas palabras y evalúa cómo cambian las predicciones. Luego, crea un modelo lineal que aproxima las decisiones locales del modelo original. El resultado puede indicar que las palabras “gratis”, “oferta” y “click” fueron las que más influyeron en que el correo fuera clasificado como spam.\n",
    "\n",
    "### **Counterfactual Explanations**\n",
    "##### **Principio de Funcionamiento**\n",
    "\n",
    "Las explicaciones contrafactuales se basan en el principio de mostrar qué cambios mínimos serían necesarios en las variables de entrada para obtener un resultado diferente del modelo. En lugar de explicar cómo el modelo llegó a una decisión, buscan identificar qué modificaciones podrían alterar esa decisión. Este tipo de explicación se centra en ejemplos “qué pasaría si”, es decir, si una persona o entidad cambia ciertos valores de sus características, cómo cambiaría la salida del modelo.\n",
    "\n",
    "##### **En que tipos de modelos se aplica**\n",
    "Las explicaciones contrafactuales se aplican principalmente a modelos supervisados, tanto de clasificación como de regresión. Son especialmente útiles en contextos donde las decisiones del modelo afectan directamente a las personas, como en la aprobación de créditos, selección de candidatos o diagnósticos médicos. Este enfoque ayuda a los usuarios a entender qué acciones concretas pueden tomar para modificar un resultado desfavorable.\n",
    "\n",
    "##### **Ejemplo**\n",
    "Si un modelo de crédito rechaza una solicitud con base en los datos de un cliente que tiene un ingreso mensual de Q3,000, una edad de 25 años y ahorros de Q500, una explicación contrafactual podría indicar que si el ingreso fuera de al menos Q4,500 o los ahorros superaran Q2,000, el préstamo habría sido aprobado. Este tipo de explicación es muy útil porque traduce las decisiones del modelo en cambios concretos y comprensibles que una persona podría realizar para lograr un resultado distinto.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
